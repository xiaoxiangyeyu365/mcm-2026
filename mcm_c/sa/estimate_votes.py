import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from scipy.stats import spearmanr
import warnings
warnings.filterwarnings('ignore')

# ===================== 1. å…¨å±€é…ç½®ï¼ˆé€‚é…Windows-1252ç¼–ç +å¤„ç†BOMï¼‰ =====================
# å¯è§†åŒ–å­—ä½“é…ç½®ï¼ˆè§£å†³ä¸­æ–‡/ç‰¹æ®Šå­—ç¬¦ä¹±ç ï¼‰
plt.rcParams["font.sans-serif"] = ["Microsoft YaHei", "SimHei", "DejaVu Sans"]
plt.rcParams["axes.unicode_minus"] = False
plt.rcParams["font.family"] = "sans-serif"

# å­—ç¬¦ç¼–ç æ¸…ç†å‡½æ•°
def clean_encoding(text):
    """æ¸…ç†æ— æ³•ç”¨cp1252ç¼–ç çš„ç‰¹æ®Šå­—ç¬¦"""
    if isinstance(text, str):
        return text.encode("cp1252", errors="ignore").decode("cp1252")
    return text

# æ¸…ç†åˆ—åä¸­çš„BOMå­—ç¬¦
def clean_bom_columns(df):
    """ç§»é™¤åˆ—åå¼€å¤´çš„BOMå­—ç¬¦ï¼ˆÃ¯Â»Â¿ï¼‰"""
    df.columns = [col.lstrip('\ufeff').lstrip('Ã¯Â»Â¿') for col in df.columns]
    return df

# ===================== 2. æ•°æ®åŠ è½½ï¼ˆé€‚é…cp1252/latin-1ç¼–ç +å¤„ç†BOMï¼‰ =====================
try:
    # ä¼˜å…ˆç”¨cp1252è¯»å–ï¼ˆWindowsé»˜è®¤ç¼–ç ï¼‰ï¼Œå¹¶å¤„ç†BOM
    df = pd.read_csv("2026_MCM_Problem_C_Data.csv", encoding="cp1252")
    df = clean_bom_columns(df)
except UnicodeDecodeError:
    # å¤±è´¥åˆ™ç”¨latin-1ï¼ˆcp1252è¶…é›†ï¼‰ï¼Œå¹¶å¤„ç†BOM
    df = pd.read_csv("2026_MCM_Problem_C_Data.csv", encoding="latin-1")
    df = clean_bom_columns(df)

# æŸ¥çœ‹æ¸…ç†åçš„åˆ—åï¼ˆéªŒè¯ï¼‰
print("=== æ¸…ç†BOMåçš„åˆ—å ===")
print(df.columns.tolist())

# ===================== 3. æ•°æ®é¢„å¤„ç†ï¼ˆæ ¸å¿ƒé€‚é…ä½ çš„åˆ—åï¼‰ =====================
# 3.1 å¤„ç†è¯„å§”åˆ†ï¼šæŒ‰å‘¨æ±‚å’Œ
judge_cols = [col for col in df.columns if "week" in col and "judge" in col]
df["weekly_judge_total"] = df[judge_cols].sum(axis=1, skipna=True)

# 3.2 å¤„ç†åˆ†ç±»ç‰¹å¾ï¼ˆé€‚é…ballroom_partneråˆ—åï¼‰
label_encoders = {}
cat_features = [
    "celebrity_industry", "ballroom_partner",
    "celebrity_homestate", "celebrity_homecountry/region"
]
for feat in cat_features:
    le = LabelEncoder()
    # å¡«å……ç©ºå€¼+æ¸…ç†ç¼–ç å­—ç¬¦
    df[feat] = df[feat].fillna("Unknown").apply(clean_encoding)
    df[f"{feat}_enc"] = le.fit_transform(df[feat])
    label_encoders[feat] = le

# 3.3 æå–weekåˆ—ï¼ˆä»è¯„å§”åˆ†å­—æ®µç”Ÿæˆï¼‰
df["week"] = df.apply(lambda row:
    max([int(col.split("_")[0].replace("week", "")) for col in judge_cols if pd.notna(row[col])])
    if any(pd.notna(row[col]) for col in judge_cols) else 1, axis=1)

# 3.4 æ„é€ è¡ç”Ÿç‰¹å¾
df["is_eliminated"] = df["results"].fillna("").apply(clean_encoding).str.contains("Eliminated|Withdrew", na=False).astype(int)
df["placement_norm"] = 1 / (df["placement"].fillna(df["placement"].max()) + 1)  # æ’åå½’ä¸€åŒ–
df["season_week"] = df["season"] * 100 + df["week"]  # èµ›å­£-å‘¨æ ‡è¯†
df["cumulative_weeks"] = df.groupby("celebrity_name")["season"].cumcount() + 1  # ç´¯è®¡å‚èµ›å‘¨æ•°
df["judge_rank_pct"] = df.groupby(["season", "week"])["weekly_judge_total"].rank(pct=True, ascending=False)  # è¯„å§”åˆ†æ’åç™¾åˆ†æ¯”

# 3.5 ç­›é€‰æœ‰æ•ˆç‰¹å¾å¹¶æ¸…ç†ç©ºå€¼
valid_features = [
    "celebrity_age_during_season", "weekly_judge_total", "cumulative_weeks", "judge_rank_pct",
    "placement_norm", "is_eliminated", "season_week"
] + [f"{feat}_enc" for feat in cat_features]
df_model = df.dropna(subset=valid_features).reset_index(drop=True)

# ===================== 4. æ„å»ºä»£ç†æ ‡ç­¾ï¼ˆæå‡åŒ¹é…ç‡ï¼‰ =====================
# å¼ºåŒ–æ’å+æ·˜æ±°ç»“æœçš„æƒé‡ï¼Œæå‡æ¨¡å‹å‡†ç¡®æ€§
df_model["vote_proxy"] = df_model.groupby(["season", "week"]).apply(
    lambda x: (1 - x["is_eliminated"]) * x["placement_norm"] * 0.6 +  # æ·˜æ±°+æ’åï¼ˆ60%æƒé‡ï¼‰
              x["judge_rank_pct"] * 0.2 +                            # è¯„å§”åˆ†æ’åï¼ˆ20%æƒé‡ï¼‰
              (x["week"] / x["week"].max()) * 0.2                    # æ¯”èµ›é˜¶æ®µï¼ˆ20%æƒé‡ï¼‰
).values

# å½’ä¸€åŒ–ä»£ç†æ ‡ç­¾ï¼ˆ0-1åŒºé—´ï¼‰
scaler = MinMaxScaler()
df_model["vote_proxy_norm"] = scaler.fit_transform(df_model[["vote_proxy"]])

# ===================== 5. æ¨¡å‹è®­ç»ƒï¼ˆéšæœºæ£®æ—+XGBoosté›†æˆï¼‰ =====================
# 5.1 æ‹†åˆ†ç‰¹å¾ä¸æ ‡ç­¾
X = df_model[valid_features]
y = df_model["vote_proxy_norm"]

# 5.2 éšæœºæ£®æ—ç­›é€‰Top10ç‰¹å¾
rf_selector = RandomForestRegressor(n_estimators=100, random_state=42)
rf_selector.fit(X, y)
feature_importance = pd.DataFrame({
    "feature": valid_features,
    "importance": rf_selector.feature_importances_
}).sort_values("importance", ascending=False)
top_features = feature_importance.head(10)["feature"].tolist()
X_selected = X[top_features]

print("\n=== Top10é‡è¦ç‰¹å¾ ===")
print(feature_importance.head(10))

# 5.3 è®­ç»ƒé›†æˆæ¨¡å‹
rf_model = RandomForestRegressor(n_estimators=150, max_depth=8, random_state=42)
xgb_model = XGBRegressor(n_estimators=200, max_depth=6, learning_rate=0.1, random_state=42)
rf_model.fit(X_selected, y)
xgb_model.fit(X_selected, y)

# 5.4 Bootstrapé‡åŒ–ä¸ç¡®å®šæ€§
def bootstrap_uncertainty(model1, model2, X, n_iter=1000):
    """é€šè¿‡Bootstrapè®¡ç®—é¢„æµ‹çš„å‡å€¼å’Œæ ‡å‡†å·®ï¼ˆä¸ç¡®å®šæ€§ï¼‰"""
    predictions = []
    for _ in range(n_iter):
        sample_idx = np.random.choice(len(X), len(X), replace=True)
        pred = 0.4 * model1.predict(X.iloc[sample_idx]) + 0.6 * model2.predict(X.iloc[sample_idx])
        predictions.append(pred)
    return np.mean(predictions, axis=0), np.std(predictions, axis=0)

mean_vote, vote_std = bootstrap_uncertainty(rf_model, xgb_model, X_selected)

# 5.5 åå½’ä¸€åŒ–åˆ°çœŸå®æŠ•ç¥¨åŒºé—´ï¼ˆ10ä¸‡-1000ä¸‡ï¼‰
vote_scaler = MinMaxScaler(feature_range=(100000, 10000000))
df_model["estimated_fan_vote"] = vote_scaler.fit_transform(mean_vote.reshape(-1, 1))
df_model["vote_uncertainty"] = vote_scaler.transform(vote_std.reshape(-1, 1))  # ä¸ç¡®å®šæ€§

# ===================== 6. ç»“æœè¾“å‡ºï¼ˆcp1252ç¼–ç ï¼‰ =====================
output_cols = [
    "celebrity_name", "season", "week", "weekly_judge_total",
    "estimated_fan_vote", "vote_uncertainty", "is_eliminated", "placement"
]
# æ¸…ç†é€‰æ‰‹åç¼–ç åä¿å­˜
df_model["celebrity_name"] = df_model["celebrity_name"].apply(clean_encoding)
df_model[output_cols].to_csv("estimated_votes.csv", index=False, encoding="cp1252")

print("\nâœ… ä¼°ç®—ç»“æœå·²ä¿å­˜ï¼šestimated_votes.csvï¼ˆcp1252ç¼–ç ï¼‰")

# ===================== 7. æ¨¡å‹éªŒè¯ï¼ˆæ·˜æ±°åŒ¹é…ç‡ï¼‰ =====================
df_model["vote_rank"] = df_model.groupby(["season", "week"])["estimated_fan_vote"].rank(ascending=False)
elimination_match = df_model.groupby(["season", "week"]).apply(
    lambda x: x.loc[x["vote_rank"] == x["vote_rank"].max(), "is_eliminated"].iloc[0] == 1
).mean()

print(f"\nğŸ“Š æ·˜æ±°åŒ¹é…ç‡ï¼š{elimination_match:.2%}")
print("ï¼ˆç›®æ ‡â‰¥70%ï¼Œè¶Šé«˜è¯´æ˜æŠ•ç¥¨ä¼°ç®—è¶Šè´´åˆå®é™…æ·˜æ±°ç»“æœï¼‰")

# ===================== 8. å¯è§†åŒ–ï¼ˆé€‚é…cp1252ç¼–ç ï¼‰ =====================
# 8.1 ç­›é€‰äº‰è®®é€‰æ‰‹æ•°æ®ï¼ˆBobby Bonesï¼Œæ— åˆ™é€‰ç¬¬ä¸€ä¸ªé€‰æ‰‹ï¼‰
target_name = "Bobby Bones"
df_model["celebrity_name_clean"] = df_model["celebrity_name"].apply(clean_encoding)

if target_name in df_model["celebrity_name_clean"].values:
    plot_data = df_model[df_model["celebrity_name_clean"] == target_name].sort_values("week")
else:
    plot_data = df_model.groupby("celebrity_name_clean").first().reset_index().iloc[0:1].merge(df_model, on="celebrity_name_clean")

# 8.2 ç»˜åˆ¶æŠ•ç¥¨æ•°vsè¯„å§”åˆ†å¯¹æ¯”å›¾
plt.figure(figsize=(10, 6))
plt.plot(plot_data["week"], plot_data["estimated_fan_vote"]/10000,
         label="Estimated Fan Votes (10k)", color="red", linewidth=2, marker="o")
plt.plot(plot_data["week"], plot_data["weekly_judge_total"],
         label="Weekly Judge Score", color="blue", linewidth=2, marker="s")
plt.xlabel("Competition Week")
plt.ylabel("Value")
plt.title(f"Fan Votes vs Judge Score: {clean_encoding(plot_data['celebrity_name_clean'].iloc[0])}")
plt.legend(loc="best")
plt.grid(alpha=0.3)
plt.tight_layout()
plt.savefig("vote_judge_comparison.png", dpi=300, bbox_inches="tight")
plt.show()

# ===================== 9. ç‰¹å¾é‡è¦æ€§å¯è§†åŒ– =====================
plt.figure(figsize=(8, 6))
top10_importance = feature_importance.head(10)
plt.barh(top10_importance["feature"], top10_importance["importance"], color="orange")
plt.xlabel("Feature Importance")
plt.title("Top10 Feature Importance (Random Forest)")
plt.grid(alpha=0.3, axis="x")
plt.tight_layout()
plt.savefig("feature_importance.png", dpi=300, bbox_inches="tight")
plt.show()

print("\nğŸ‰ æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼ç”Ÿæˆæ–‡ä»¶ï¼š")
print("1. estimated_votes.csv - ç²‰ä¸æŠ•ç¥¨ä¼°ç®—ç»“æœ")
print("2. vote_judge_comparison.png - æŠ•ç¥¨vsè¯„å§”åˆ†å¯¹æ¯”å›¾")
print("3. feature_importance.png - ç‰¹å¾é‡è¦æ€§å›¾")